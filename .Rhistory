fit3 <- geeglm(Ysum ~ Base + Age + Trt, data = breslow,
family = poisson, id = id, corstr = "exchangeable")
View(breslow)
# 3. 使用广义线性混合模型
library(lme4)
fit4 <- glmer(Ysum ~ Base + Age + Trt + (1 | id),
data = breslow.dat, family = poisson)
fit3 <- geeglm(Ysum ~ Base + Age + Trt, data = breslow,
family = poisson, corstr = "exchangeable")
# 2. 使用拟Poissom回归
fit.od <- glm(Ysum ~ Base + Age + Trt,
data = breslow,
family = quasipoisson)
summary(fit.od)
poisgof(fit.od)
poisgof(fit.nb)
poisgof(fit2)
# discriminant analysis
# 距离判别
data(iris)
cor(iris[, 1:4])
m.setosa <- colMeans(iris[1:50, 1:4])
m.setosa
m.versicolor <- colMeans(iris[51:100, 1:4])
m.versicolor
m.virginica <- colMeans(iris[101:150, 1:4])
m.virginica
install.packages("biotools")
library(biotools)
boxM(iris[, -5], iris[, 5])
# 计算协方差矩阵
v.setosa <- cov(iris[1:50, 1:4])
v.setosa
v.versicolor <- cov(iris[51:100, 1:4])
v.versicolor
v.virginica <- cov(iris[101:150, 1:4])
v.virginica
mahalanobis(iris[1, 1:4], m.setosa, v.setosa)
mahalanobis(iris[1, 1:4], m.versicolor, v.versicolor)
mahalanobis(iris[1, 1:4], m.virginica, v.virginica)
d.setosa <- mahalanobis(iris[, 1:4], m.setosa, v.setosa)
d.versicolor <- mahalanobis(iris[, 1:4], m.versicolor, v.versicolor)
d.virginica <- mahalanobis(iris[, 1:4], m.virginica, v.virginica)
d <- data.frame(d.setosa, d.versicolor, d.virginica)
head(d)
index <- apply(d, MARGIN = 1, FUN = which.min)
index
type <- factor(index, labels = c("setosa", "versicolor", "virginica"))
type
table(type, iris$Species)
which(type == "virginica" & iris$Species == "versicolor")
# K最邻近判别
set.seed(1234)
nrow(iris)
s <- sample(1:150, 100)
train <- iris[s, ]
test <- iris[-s, ]
cl <- train[, 5]
library(class)
iris.knn <- knn(train[, -5], test[, -5], cl)
iris.knn
confusion.matrix <- table(iris.knn, test[, 5])
confusion.matrix
accuracy <- sum(diag(confusion.matrix)) / nrow(test)
accuracy
47/50
accuracy <- vector(length = 20)
accuracy[i] <- sum(diag(confusion.matrix)) / nrow(test)
accuracy[k] <- sum(diag(confusion.matrix)) / nrow(test)
i <- NULL
accuracy <- vector(length = 20)
for (i in 1:20) {
iris.knn <- knn(train[, -5], test[, -5], cl, k = i)
confusion.matrix <- table(iris.knn, test[, 5])
accuracy[i] <- sum(diag(confusion.matrix)) / nrow(test)
}
accuracy
plot(accuracy, type = "b", xlab = "k", ylab = "accuracy")
# Fisher判别
library(MASS)
iris.ld <- lda(Species ~ ., data = iris)
iris.ld
iris.pred <- predict(iris.ld)
iris.pred$class
table(iris.pred$class, iris$Species)
which(iris.pred$class == "versicolor" & iris$Species == "virginica")
which(iris.pred$class == "virginica" & iris$Species == "versicolor")
iris.pred$x
LD1 <- iris.pred$x[, 1]
LD2 <- iris.pred$x[, 2]
col <- as.numeric(iris$Species)
pch <- as.numeric(iris$Species)
plot(LD1, LD2, col = col, pch = pch, xlab = "LD1", ylab = "LD2")
legend("topright", legend = levels(iris$Species), col = 1:3, pch = 1:3)
legend("topleft", legend = levels(iris$Species), col = 1:3, pch = 1:3)
legend("topright", legend = levels(iris$Species), col = 1:3, pch = 1:3)
plot(LD1, LD2, col = col, pch = pch, xlab = "LD1", ylab = "LD2")
legend("topright", legend = levels(iris$Species), col = 1:3, pch = 1:3)
plot(LD1, LD2, col = col, pch = pch, xlab = "LD1", ylab = "LD2")
legend("topright", legend = levels(iris$Species), col = 1:3, pch = 1:3)
points(LD1[c(71, 84)], LD2[c(71, 84)], cex = 2)
points(LD1[134], LD2[134], cex = 2)
install.packages("klaR")
# Bayes判别
library(klaR)
names(iris.bayes)
iris.bayes <- NaiveBayes(Species ~ ., data = iris)
names(iris.bayes)
plot(iris.bayes)
iris.bayes.pred <- predict(iris.bayes, iris[, -5])
table(iris.bayes.pred$class, iris$Species)
# 主成分分析
cor.matrix <- Harman23.cor$cov
eigen(cor.matrix)
# 计算特征值和特征向量
eigen(cor.matrix)$values
round(eigen(cor.matrix)$vectors, 3)
# 上述计算过程也可以用
PCA <- princomp(covmat = cor.matrix)
summary(PCA, loadings = T)
screen(PCA, type = "lines")
screen(PCA)
screeplot(PCA, type = "lines", main = "Screeplot of PCA")
abline(h = 1)
load <- loadings(PCA)
plot(load[, 1:2], xlim = c(-0.5, 0.5), ylim = c(-0.5, 0.5),
xlab = "PC1", ylab = "PC2")
text(load[, 1], load[, 2], adj = c(-0.3, 0)
text(load[, 1], load[, 2], adj = c(-0.3, 0))
abline(h = 0, v = 0, lty = 2)
# 因子分析
library(psych)
fa.parallel(Harman23.cor$cov, n.iter = 100, fa = "both",
n.obs = 305, main = "Parallel Analysis Scree Plots")
fa1 <- fa(Harman23.cor$cov, nfactors = 3, rotate = "varimax",
fm = "minres")
fa1
fa1$loadings
fa1 <- fa(Harman23.cor$cov, nfactors = 3, rotate = "varimax",
fm = "minres")
fa1
fa1$loadings
fa.diagram(fa1, simple = TRUE, cut = 0.3)
fa2 <- fa(Harman23.cor$cov, nfactors = 3, rotate = "oblimin",
fm = "minres")
fa2
fa2$loadings
fa.diagram(fa2, simple = TRUE, cut = 0.3)
fa3 <- fa(Harman23.cor$cov, nfactors = 3, rotate = "promax",
fm = "minres")
fa3
fa3$loadings
fa.diagram(fa3, simple = TRUE, cut = 0.3)
fa4 <- fa(Harman23.cor$cov, nfactors = 3, rotate = "varimax",
fm = "ml")
fa4
fa4$loadings
fa.diagram(fa4, simple = TRUE, cut = 0.3)
fa5 <- fa(Harman23.cor$cov, nfactors = 3, rotate = "oblimin",
fm = "ml")
fa5
fa5$loadings
fa.diagram(fa5, simple = TRUE, cut = 0.3)
# 二分类结果的评价指标
library(devtools)
devtools::install_github("gadenbuie/rsthemes")
?rsthemes::theme_rsthemes
library(rthemes)
# 二分类结果的评价指标
library(devtools)
devtools::install_github("gadenbuie/rsthemes")
# 假设在一项研究中，有100名患者和100名非患者，某一检测试验的灵敏度是80%，特异度
# 是90%，数据可以描述如下：
table1 <- as.table(cbind(c(80, 20), c(10, 90)))
dimnames(table1) <- list("test" = c("positive", "negative"),
"disease" = c("present", "absent"))
table1
mosaicplot(table1,
main = "Mosaic plot of test results",
xlab = "Test result",
ylab = "Disease status",
color = TRUE)
# 计算灵敏度、特异度、阳性预测值和阴性预测值
sensitivity <- table1[1, 1] / sum(table1[1, ])
sensitivity
specificity <- table1[2, 2] / sum(table1[2, ])
specificity
ppv <- table1[1, 1] / sum(table1[, 1])
ppv
npv <- table1[2, 2] / sum(table1[, 2])
npv
# 计算阳性似然比和阴性似然比
positive_likelihood_ratio <- sensitivity / (1 - specificity)
positive_likelihood_ratio
negative_likelihood_ratio <- (1 - sensitivity) / specificity
negative_likelihood_ratio
# 计算准确率
accuracy <- sum(diag(table1)) / sum(table1)
accuracy
# 计算F1分数
f1_score <- 2 * (ppv * sensitivity) / (ppv + sensitivity)
f1_score
# 计算ROC曲线下面积（AUC）
library(pROC)
install.packages("pROC")
# 计算ROC曲线下面积（AUC）
library(pROC)
roc_obj <- roc(c(rep(1, 80), rep(0, 20)),
c(rep(1, 10), rep(0, 90)))
roc_obj
plot(roc_obj,
main = "ROC curve",
col = "blue",
lwd = 2)
auc(roc_obj)
# 计算Youden指数
youden_index <- sensitivity + specificity - 1
youden_index
# 计算Kappa系数
library(irr)
install.packages("irr")
# 计算Kappa系数
library(irr)
kappa_value <- kappa2(table1)
kappa_value
# 计算Bland-Altman图
library(ggplot2)
library(ggpubr)
library(dplyr)
# 生成数据
set.seed(123)
n <- 100
x <- rnorm(n, mean = 50, sd = 10)
y <- x + rnorm(n, mean = 0, sd = 5)
data <- data.frame(x, y)
# 计算均值和差值
data <- data %>%
mutate(mean = (x + y) / 2,
diff = x - y)
# 绘制Bland-Altman图
ggplot(data, aes(x = mean, y = diff)) +
geom_point() +
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
geom_hline(yintercept = mean(data$diff), linetype = "dashed", color = "blue") +
geom_hline(yintercept = mean(data$diff) + 1.96 * sd(data$diff), linetype = "dashed", color = "green") +
geom_hline(yintercept = mean(data$diff) - 1.96 * sd(data$diff), linetype = "dashed", color = "green") +
labs(title = "Bland-Altman plot",
x = "Mean of two measurements",
y = "Difference between two measurements")
# 计算一致性相关系数（ICC）
library(psych)
icc_value <- ICC(data[, c("x", "y")])
icc_value
# 单个ROC分析
library(pROC)
data("aSAH")
str(aSAH)
roc1 <- roc(outcome ~ s100b, data = aSAH))
roc1 <- roc(outcome ~ s100b, data = aSAH)
attributes(roc1)
roc1$auc
roc.result <- data.frame(threshold = roc1$thresholds,
sensitivity = roc1$sensitivities,
specificity = roc1$specificities)
roc.result$youden <- roc.result$sensitivity + roc.result$specificity - 1
head(roc.result)
which.max(roc.result$youden)
roc.result[18, ]
# 实际上，也可以
coords(roc1, "best", transpose = F)
plot(1 - roc1$specificities,
roc1$sensitivities,
type = "l",
lwd = 2,
xlab = "1 - Specificity",
ylab = "Sensitivity",
main = "ROC Curve")
abline(0, 1, lty = 2)
# 计算AUC
roc1$auc
roc1$ci
roc1$ci[1]
roc1$ci[2]
# 计算AUC
roc1$auc
plot.roc(roc1,
print.auc = TRUE,
auc.polygon = T,
grid = c(0.1, 0.2),
grid.col = c("green", "red"),
auc.polygon.col = "lightblue",
print.thres = TRUE,
main = "ROC Curve for s100b")
ci.auc(roc1, conf.level = 0.95)
# 2个ROC曲线的比较
roc1 <- roc(aSAH$outcome, aSAH$s100b)
roc2 <- roc(aSAH$outcome, aSAH$ndka)
roc.test(roc1, roc2, method = "delong"))
roc.test(roc1, roc2, method = "delong")
plot(roc1, col = "blue", lwd = 2)
plot(roc2, col = "red", lwd = 2, add = TRUE)
legend("bottomright",
legend = c("s100b", "ndka"),
col = c("blue", "red"),
lwd = 2)
# logistic回归的ROC曲线
fit <- glm(case ~ induced + spontaneous, family = binomial, data = infert)
library(epiDisplay)
logistic.display(fit)
lroc(fit, line.col = "red", lwd = 3)
# 联合试验
cut.point1 <- coords(roc1, "best", transpose = F)$threshold
cut.point2 <- coords(roc2, "best", transpose = F)$threshold
cut.point1
cut.point2
parallel <- ifelse(aSAH$s100b > cut.point1 | aSAH$ndka > cut.point2,
"positive", "negative")
table(parallel, aSAH$outcome)
serial <- ifelse(aSAH$s100b > cut.point1 & aSAH$ndka > cut.point2,
"positive", "negative")
table(serial, aSAH$outcome)
sensitivity_parallel
# 计算联合试验的灵敏度、特异度、阳性预测值和阴性预测值
sensitivity_parallel <- sum(parallel == "positive" & aSAH$outcome == 1) / sum(aSAH$outcome == 1)
sensitivity_parallel
specificity_parallel <- sum(parallel == "negative" & aSAH$outcome == 0) / sum(aSAH$outcome == 0)
specificity_parallel
ppv_parallel <- sum(parallel == "positive" & aSAH$outcome == 1) / sum(parallel == "positive")
ppv_parallel
npv_parallel <- sum(parallel == "negative" & aSAH$outcome == 0) / sum(parallel == "negative")
sensitivity_serial <- sum(serial == "positive" & aSAH$outcome == 1) / sum(aSAH$outcome == 1)
sensitivity_serial
specificity_serial <- sum(serial == "negative" & aSAH$outcome == 0) / sum(aSAH$outcome == 0)
specificity_serial
install.packages("meta")
library(meta)
data("Fleiss93")
# Fleiss93数据集是一个包含多个研究的二分类变量数据集
# 其中每个研究都有一个治疗组和一个对照组的事件数和总人数
# 该数据集用于进行meta分析
# 该数据集包含了93个研究的结果
Fleiss93
# 合并OR值
metabin(event.e, n.e, event.c, n.c,
data = Fleiss93,
sm = "OR",
method = "Inverse",
studlab = paste(author, year),
comb.fixed = FALSE,
comb.random = TRUE,
prediction = TRUE,
hakn = TRUE,
title = "Meta-analysis of binary outcomes",
xlab = "Odds Ratio",
col.diamond = "blue",
col.diamond.lines = "black",
col.by = "red",
col.random = "green",
col.fixed = "orange")
# 合并OR值
metabin(event.e, n.e, event.c, n.c,
data = Fleiss93,
sm = "OR")
m <- metabin(event.e, n.e, event.c, n.c,
data = Fleiss93,
sm = "OR",
studlab = paste(author, year))
names(Fleiss93)
m <- metabin(event.e, n.e, event.c, n.c,
data = Fleiss93,
sm = "OR",
studlab = paste(study, year))
forest(m, comb.random = F)
forest(m, comb.common = F)
forest(m, comb.fixed = F)
forest(m)
metabin(event.e, n.e, event.c, n.c,
data = Fleiss93,
sm = "RR",
studlab = paste(study, year)) # 显示研究编号和研究年份
# 合并RD值
metabin(event.e, n.e, event.c, n.c,
data = Fleiss93,
sm = "RD",
studlab = paste(study, year)) # 显示研究编号和研究年份
# 发表偏倚的识别
# 最常用的方法是作漏斗图（funnel plot）
# 漏斗图是一个散点图，横坐标是效应量，纵坐标是标准误
# 如果漏斗图呈现对称的漏斗形状，说明没有发表偏倚
# 如果漏斗图呈现不对称的漏斗形状，说明有发表偏倚
funnel(m)
# 发表偏倚的检验
metabias(m)
metabias(m, k.min = 7)
# 用剪补法评估(trim and fill method)发表偏倚，方法同基于漏斗图的对称原理
# 首先剪切掉漏斗图中不对称的小样本研究，用剪切后的对称部分估计漏斗图的中心值，
# 再在漏斗图的中心值两侧添补被剪切部分以及相应的缺失部分，最后用添补后的漏斗图
# 重新估计校正后的效应量
tf1 <- trimfill(m)
summary(tf1)
funnel(tf1)
# 敏感性分析
# 敏感性分析是指通过改变某些参数或假设来评估结果的稳健性和可靠性
metainf(m, pooled = "common")
# 连续性变量资料的meta分析
data("Fleiss93cont")
Fleiss93cont
metacont(n.e, mean.e, sd.e,
n.c, mean.c, sd.c,
data = Fleiss93cont,
sm = "SMD",
studlab = paste(study, year)) # 显示研究编号和研究年份
reticulate::repl_python()
load("D:/python/R_and_python_book/r_and_python_book-master/01-intro.Rmd")
47 / 450
139 / 450
450 * 0.2
450 * 0.25
typeof('1')
typeof(1)
typeof(TRUE)
typeof(1.2)
a <- 2
class(a)
install.packages("dslabs")
library(dslabs)
data(murders)
class(murders)
install.packages("palmerpenguins")
install.packages("quarto")
quarto::quarto_render("hello.qmd")
getwd()
setwd("D:/Users/hzw/Desktop/reader/知识的海洋/programming/R/CSDN合集/R 语言数据科学/draft")
getwd()
quarto::quarto_render("hello.qmd")
#| label: plot-penguins
#| warning: false
#| echo: false
ggplot(penguins,
aes(x = flipper_length_mm, y = bill_length_mm)) +
geom_point(aes(color = species, shape = species)) +
scale_color_manual(values = c("darkorange","purple","cyan4")) +
labs(
title = "Flipper and bill length",
subtitle = "Dimensions for penguins at Palmer Station LTER",
x = "Flipper length (mm)", y = "Bill length (mm)",
color = "Penguin species", shape = "Penguin species"
) +
theme_minimal()
#| label: load-packages
#| include: false
library(tidyverse)
library(palmerpenguins)
#| label: plot-penguins
#| warning: false
#| echo: false
ggplot(penguins,
aes(x = flipper_length_mm, y = bill_length_mm)) +
geom_point(aes(color = species, shape = species)) +
scale_color_manual(values = c("darkorange","purple","cyan4")) +
labs(
title = "Flipper and bill length",
subtitle = "Dimensions for penguins at Palmer Station LTER",
x = "Flipper length (mm)", y = "Bill length (mm)",
color = "Penguin species", shape = "Penguin species"
) +
theme_minimal()
#| label: load-packages
#| echo: false
library(ggplot2)
#| label: load-packages
#| echo: false
library(ggplot2)
#| label: load-packages
#| echo: false
library(ggplot2)
#| label: scatterplot
#| echo: true
ggplot(mpg, aes(x = hwy, y = cty, color = cyl)) +
geom_point(alpha = 0.5, size = 2) +
scale_color_viridis_c() +
theme_minimal()
#| label: scatterplot
#| echo: true
ggplot(mpg, aes(x = hwy, y = cty, color = cyl)) +
geom_point(alpha = 0.5, size = 2) +
scale_color_viridis_c() +
theme_minimal()
#| label: fig-mpg
#| fig-cap: "City and highway mileage for 38 popular models of cars."
#| fig-subcap:
#|   - "Color by number of cylinders"
#|   - "Color by engine displacement, in liters"
#| layout-ncol: 2
#| column: page
ggplot(mpg, aes(x = hwy, y = cty, color = cyl)) +
geom_point(alpha = 0.5, size = 2) +
scale_color_viridis_c() +
theme_minimal()
ggplot(mpg, aes(x = hwy, y = cty, color = displ)) +
geom_point(alpha = 0.5, size = 2) +
scale_color_viridis_c(option = "E") +
theme_minimal()
getwd()
quarto::quarto_render(
"authoring.qmd",
output_format = c("html", "pdf", "docx")
)
quarto::quarto_render(
"computations.qmd",
output_format = c("html", "pdf", "docx")
)
